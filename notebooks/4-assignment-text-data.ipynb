{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3625f252-701b-4821-a120-424a22a963cb",
   "metadata": {},
   "source": [
    "# SICSS Zurich Day 4 Assignment: Text as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269795e-2716-4233-af4a-ee9c38bde2db",
   "metadata": {},
   "source": [
    "# 1. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a552f37-a5d1-4a1f-98a0-b959f9e952e2",
   "metadata": {},
   "source": [
    "In this assignment, we will explore how to load a text classification dataset (AG's news, originally posted [here](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)), how we can preprocess the data and extract useful information from a real-world dataset. First, we have to download the data; we only download a subset of the data with four classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e258a21-a996-46cd-8da6-c0465f22ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-16 19:24:37--  https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29470338 (28M) [text/plain]\n",
      "Saving to: ‘train.csv’\n",
      "\n",
      "train.csv           100%[===================>]  28.10M  35.8MB/s    in 0.8s    \n",
      "\n",
      "2021-06-16 19:24:40 (35.8 MB/s) - ‘train.csv’ saved [29470338/29470338]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8906d7f-97b4-4cd7-9c83-556598a66823",
   "metadata": {},
   "source": [
    "## Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab84436-abcb-4407-97a3-5053132eabf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120000 entries, 0 to 119999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   0       120000 non-null  int64 \n",
      " 1   1       120000 non-null  object\n",
      " 2   2       120000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1  \\\n",
       "0  3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1  3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2  3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3  3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4  3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                                   2  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train.csv\", header=None)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46e99f-6302-49cf-b5f4-c210f39dd96d",
   "metadata": {},
   "source": [
    "Let's make the data more human readable by adding a header and replacing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d4cd709-b51f-4466-9dc7-bd14bac84668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca162a6-6667-4760-acc4-1b5caeccc0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              title  \\\n",
       "0  business  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1  business  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2  business    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3  business  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4  business  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                                lead  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261ec9a-d8c8-4f57-8a32-1e299eb01129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement a new column text which contains the lowercased title and lead\n",
    "df[\"text\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d424f09-d00d-4b05-a076-62e304f3c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO print the number of documents for each label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491b2f1-1d46-4834-bb00-4d470e955d20",
   "metadata": {},
   "source": [
    "## Document Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6129181-45f7-4fcc-8057-ea88fcf84d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create a new column with the number of words for each text\n",
    "# TODO plot the average number of words per label "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80378f51-dfdb-4c56-ae33-ca6b055a8ddf",
   "metadata": {},
   "source": [
    "## Word Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e4cb7-5903-4343-8458-89c4dccce41c",
   "metadata": {},
   "source": [
    "Let's implement a keyword search (similar to the baker-bloom economic uncertainty) and compute how often some given keywords (\"play\", \"tax\", \"blackberry\", \"israel\") appear in the different classes in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344011b-6276-4a07-9f58-442589cbebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "keywords = [\"play\", \"tax\", \"blackberry\", \"israel\"]\n",
    "for keyword in keywords:\n",
    "    #TODO implement a regex pattern\n",
    "    x = ...\n",
    "    pattern = re.compile(x)\n",
    "    def count_keyword_frequencies(x):\n",
    "        #TODO implement a function which counts how often a pattern appears in a text\n",
    "        num_occurrences = ...\n",
    "        return num_occurrences\n",
    "    # Now, we can print how often a keyword appears in the data\n",
    "    print (df[\"text\"].apply(count_keyword_frequencies).sum())\n",
    "    # and we want to find out how often the keyword appears withhin each class\n",
    "    for label in df[\"label\"].unique():\n",
    "        print (\"label:\", label,\", keyword:\", keyword)\n",
    "        #TODO print how often the keyword appears in this class\n",
    "    print (\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e8ee4-5c12-4f04-878c-4bd3b5030130",
   "metadata": {},
   "source": [
    "As a last exercise, we re-use the fuzzy keyword search implemented above and plot the total number of occurrences of \"tax\" (and it's variations, e.g. taxation, taxes etc.) for each class in the dataset. Hint: have a look at the [pandas bar plot with group by](https://queirozf.com/entries/pandas-dataframe-plot-examples-with-matplotlib-pyplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc283b9f-3b7b-4d8a-aeaf-bc562d863c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "keyword = \"tax\"\n",
    "pattern = re.compile(...)\n",
    "\n",
    "def count_keyword_frequencies(x):\n",
    "    #TODO implement a function which counts the total number of the word \"tax\" (and other fuzzy matches of tax) appearing in a given text\n",
    "\n",
    "df[\"counts\"] = df[\"text\"].apply(count_keyword_frequencies)\n",
    "#TODO create a bar plot for the wordcounts of \"tax\" for each class in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbbcdb-96e9-46d4-9f60-ffec61fd5ea4",
   "metadata": {},
   "source": [
    "# 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d85425-72d9-471f-8218-ea480fd7498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32966d-4825-4fcc-ad53-aa493ed6c5b6",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77355105-33eb-4a30-99e0-72161acaf9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "dfs = df.sample(50)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "##TODO use spacy to split the documents in the sampled dataframe (dfs) in sentences and tokens\n",
    "##TODO print the first sentence of the first document in your sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86655c8-b530-4af4-98ba-1d000e295b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO create a new column with tokens in lowercase (x.lower()), without punctuation tokens (x.is_punct) nor stopwords (x.is_stop)\n",
    "##TODO print the tokens (x.lemma_) and the tags (x.tag_ ) of the first sentence of the first document (doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719584d5-b711-4640-92ad-769bacbb8378",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "Let's compute the ratio of named entities starting with a capital letter, e.g. if we have \"University of Chicago\" as a NE, \"University\" and \"Chicago\" are capitalized, \"of\" is not, thus the ratio is 2/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820a693-1955-4c95-a3af-613b791c8dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print the ratio of tokens being part of a named entity span starting with a capital letter (doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d454c-efc1-430e-9544-cac67fa78137",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print the ratio of capitalized tokens not being part of a named entity span\n",
    "# e.g. \"The dog barks\" = 1/3; 3 tokens, only \"The\" is capitalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ab11e-c954-49d4-b74e-5b6bb4eaf759",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print the ratio of capitalized tokens not being a named entity and not being the first token in a sentence\n",
    "# e.g. \"The dog barks\" = 0; 3 tokens, \"The\" is capitalized but the starting token of a sentence, no other tokens are capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0bfba-8fd1-429a-9628-05d28c5325ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3aa7f-4d31-442a-8ceb-855931d181a6",
   "metadata": {},
   "source": [
    "Give an example of a capitalized token in the data which is neither a named entity nor at the start of a sentence. What could be the reason the token is capitalized (one sentence)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d0e98-3460-4bc1-8e02-6034a33f85ac",
   "metadata": {},
   "source": [
    "## Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf6a41-1d40-4a32-abce-dfb2ca23fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=0.01, \n",
    "                        max_df=0.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        use_idf=True, # the new piece\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##TODO using the whole sample, produce a world cloud with bigrams for each label using tfidf frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141a5b2-2d5b-4155-b95c-4e6401b86a47",
   "metadata": {},
   "source": [
    "## Supervised Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7d790-a44b-4f54-825a-9438ce2c4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "\n",
    "##TODO compute the number of words per document (excluding stopwords)\n",
    "##TODO get the most predictive features of the number of words per document using first f_class and then chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aae32c-03ce-469b-8dab-6c9f1dffc601",
   "metadata": {},
   "source": [
    "# Distance and Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6db681-efbd-48af-8cad-dfa3d132f19a",
   "metadata": {},
   "source": [
    "## Load and Pre-process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdc1ee-f24f-415e-8709-f5fb42924364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883c7b3-abff-43dd-b1fa-9853d8a130d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "dfs = df.sample(200)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "##TODO pre-process text as you did in HW02\n",
    "##TODO vectorize the pre-processed text using CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6e88d-1488-4981-ab5c-a5e05385bd1b",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab18c82e-a32c-40f9-b040-57aabcc9274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "##TODO compute the cosine similarity for the first 200 snippets and for the first snippet, show the three most similar snippets and their respective cosine similarity scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8403431e-8b71-407b-99af-e6366d43d785",
   "metadata": {},
   "source": [
    "## Topic Modeling: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b2d8c-453e-43c2-acd5-aaf9e557f09c",
   "metadata": {},
   "source": [
    "For this part you will need to use LDA Mallet. If you cannot have Mallet run, you can use the simple LDA algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17ed13-460d-4343-8c97-e92c08b301c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "##TODO create a dictionary with the pre-processed tokenized text and filter it according to frequencies and keeping 1000 vocabularies\n",
    "##TODO create the doc_term_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5714b3d-b8e8-4046-8e65-222c67921022",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO train a LDA Mallet model with 5, 10 and 15 topics\n",
    "##TODO compute the coherence score for each of these model and print the topics from the model with highest coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db756f7-29f2-4dc7-99f6-aa153ec6bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "##TODO using LDAvis visualize the topics using the optimal number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d416ba7-3bfb-4e1e-9aef-c33fdcbc76d5",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a69919-c2eb-444d-938c-f0027b5f0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "api.info(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2960ac8f-9b88-4c58-af0e-4194870218e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01290c1d-4d8c-4527-8c88-9ba32f417222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "##TODO train a word2vec model on this dataset, only consider words which appear at least 10 times in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93031a-d119-4e62-a439-abe8a9bf76bf",
   "metadata": {},
   "source": [
    "**Word Similarities**\n",
    "\n",
    "gensim models provide almost all the utility you might want to wish for to perform standard word similarity tasks. They are available in the .wv (wordvectors) attribute of the model, more details could be found [here](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52452a7c-4858-4ccb-843c-ec624897949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv\n",
    "\n",
    "##TODO find the closest words to king"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48743404-b5e3-4501-a4ae-6336ae6d6d57",
   "metadata": {},
   "source": [
    "King is to man as woman is to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba321be0-e1f2-43c0-b29d-dac3f6a69db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO find the closest word for the vector \"woman\" + \"king\" - \"man\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b31e80-0ccb-4fd0-b506-d5565f756fd8",
   "metadata": {},
   "source": [
    "**Evaluate Word Similarities** \n",
    "\n",
    "One common way to evaluate word2vec models are word analogy tasks. Let's check how good our model is on one of those. We consider the [WordSim353](http://alfonseca.org/eng/research/wordsim353.html) benchmark, the task is to determine how similar two words are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa4ae1-6e92-49d0-a10a-bd6a0e7f33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
    "!tar xf ws353simrel.tar.gz\n",
    "\n",
    "path = \"wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n",
    "\n",
    "def load_data(path):\n",
    "    X, y = [], []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            X.append((line[0], line[1])) # each entry in x contains two words, e.g. X[0] = (tiger, cat)\n",
    "            y.append(float(line[-1])) # each entry in y is the annotation how similar two words are, e.g. Y[0] = 7.35\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data(path)\n",
    "print (X[:3], y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fcb40-3308-43ab-8915-504a83201239",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO compute how similar the pairs in the WordSim353 are according to our model\n",
    "##TODO if  aword is not present in our model, we assign similarity 0 for the respective text pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48275f9e-ba85-45c9-b117-53c4d0c77e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "##TODO compute spearman's rank correlation between our prediction and the human annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92575ed-0fb7-4a40-a026-4c604f2be92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "\n",
    "##TODO compute word similarities in the WordSim353 dataset using spaCy word embeddings\n",
    "##TODO compute spearman's rank correlation between these similarities and the human annotations\n",
    "# Don't worry if results are not too convincing for this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9bfa8-c500-4665-b0e1-6430c34a4db9",
   "metadata": {},
   "source": [
    "# Practice with Causal Graphs\n",
    "\n",
    "**Think of two example causal inference questions:**\n",
    "\n",
    "1. where you have language as an outcome \n",
    "\n",
    "2. where you have language as a treatment\n",
    "\n",
    "**Try to personalize it based on one of your research projects**\n",
    "\n",
    "\n",
    "Causal graph template:\n",
    "https://docs.google.com/drawings/d/1zpo-1TPAKKTggPhrzrD6N6YLI1MSr5gOrsal1NBal8k/edit?usp=sharing\n",
    "\n",
    "\n",
    "– make a copy, fill it in\n",
    "\n",
    "– make your doc viewable and paste link here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21e243-117a-4fbd-9ef9-701824b8493a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
