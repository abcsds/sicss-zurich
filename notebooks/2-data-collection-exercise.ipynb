{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection \n",
    "-- [SICSS ZÃ¼rich 2021](https://github.com/computational-social-science-zurich/sicss-zurich) -- \n",
    "\n",
    "\n",
    "## Preambule\n",
    "\n",
    "### Guidelines\n",
    "\n",
    "**In this exercise, you have a lot of freedom.** You should propose an original data collection task: <span style='color:green'> choose a website that you want to harvest, briefly justifying why you think it is interesting. For example, you can think of a data that you might want to use in a research project. If you do succeed in getting a final product by the end of the afternoon, this is okay too: it is already great if you can get a clear roadmap, with some parts already coded. </span>.\n",
    "\n",
    "The  <span style='color:green'> following guidelines </span> are indications, that you can twist depending on your specific collection task. Nonetheless, <span style='color:green'> your code should contain:</span>\n",
    "\n",
    "   1. a list of `url` that you are interested in and whose html content share a common structure (so that you can loop over them)\n",
    "   2. A `loop` over the url: the **automation** part that we care about\n",
    "   3. A parser of the text information\n",
    "   3. A **structured** output (`dataframe` or `json`): you should almost be able to run a regression directly from the dataset.     \n",
    "   \n",
    "**Alternatively**: you can use any "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <span style='color:green'>Choose a website that you want to harvest, briefly justifying why you think it is interesting</span>\n",
    "It should be a static website, with some interesting underlying structure that you could loop over. \n",
    "\n",
    "If you have 0 ideas, you can think about an area that you would be interested in: \n",
    "- newspapers article, \n",
    "- speeches from policymakers\n",
    "- wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  <span style='color:green'> Build the list of the `url` (or `query parameters`) that you will scrape\n",
    "</span>\n",
    "Before building the scraper, explain (& code) how you will generate the list of url that you will scrape. It can be based on a query parameters or on a list of url that are already on a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <span style='color:green'> First build the scraper for 1 webpage</span>\n",
    "- query the website using `request` \n",
    "- extract the relevant information using `beautifulsoup` $\\rightarrow$ ideally, you want a clean dataset, that could be used for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <span style='color:green'> Scraper: `loop` over the `url` list using the scraper from </span> 3. \n",
    "\n",
    "This is the key step. You should find a way to collect all the info, for example in `Datafame` or in a `json`. \n",
    "\n",
    "In a real-world project, you might want to:\n",
    "- save the `html` on disk before cooking the soup. \n",
    "- save each output instead of concatenating them together in a `Datafame` or in a `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <span style='color:green'> Save your file on disk</span> (Does not have to be included in the submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
